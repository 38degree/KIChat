# Deployment-Checkliste f√ºr Ubuntu/DGX Spark

## Phase 1: Hardware-Vorbereitung ‚òê

- [ ] Ubuntu 20.04 LTS oder neuer installiert
- [ ] NVIDIA GPU Driver installiert (`nvidia-smi` funktioniert)
- [ ] Internet-Verbindung verf√ºgbar
- [ ] ~200GB freier Speicher (Modelle + Daten)
- [ ] RAM: mind. 16GB, besser 32GB+

## Phase 2: Docker Setup ‚òê

```bash
chmod +x scripts/install_ubuntu.sh
./scripts/install_ubuntu.sh
```

Nach Script:
- [ ] Docker installiert (`docker --version`)
- [ ] NVIDIA Container Toolkit installiert
- [ ] GPU-Zugang funktioniert (`docker run --rm --gpus all ...`)
- [ ] Benutzer zur docker-Gruppe hinzugef√ºgt (muss ab/anmelden)

## Phase 3: HuggingFace Token ‚òê

- [ ] Token generiert auf https://huggingface.co/settings/tokens
- [ ] `.env` Datei mit Token gef√ºllt
- [ ] Token im Format `hf_xxxxx` (nicht `your_token_here`)

```bash
# Pr√ºfen:
grep "HF_TOKEN=hf_" .env
```

## Phase 4: Projekt-Setup ‚òê

```bash
# Im Projekt-Verzeichnis:
cd /path/to/NvidiaKI

# (Wird automatisch durch install_ubuntu.sh gemacht:)
mkdir -p models/{llm,whisper,tts,embedding,ocr,denoiser}
mkdir -p data/{vectordb,documents,audio,transcripts,webui,calibration}
chmod +x scripts/*.sh
```

- [ ] Verzeichnisse vorhanden
- [ ] Scripts ausf√ºhrbar

## Phase 5: Optional ‚Äî LLM Quantisierung  ‚òê

_(Skip wenn keine Quantisierung n√∂tig; kostet ~1-2h)_

```bash
./scripts/quantize_model.sh
./scripts/evaluate_quantization.sh
```

- [ ] Quantisierung abgeschlossen
- [ ] Evaluation durchgef√ºhrt
- [ ] Qualit√§t akzeptabel

## Phase 6: Container starten ‚òê

```bash
docker compose up -d
```

Monitoring beim Start:
```bash
# In anderem Terminal:
docker compose logs -f
```

**Wartezeiten (ca.):**
- vLLM startet ‚Üí 2-3 Min
- Backend initialisiert ‚Üí 1 Min
- TTS l√§dt ‚Üí 1 Min
- Total: ~5 Min

Zeichen dass bereit:
```
vllm        | INFO:     Application startup complete
backend     | INFO: Application startup complete
tts         | INFO: Application startup complete
```

- [ ] Alle Services starten ohne Fehler
- [ ] Keine "Out of Memory" Fehler
- [ ] Keine "Connection refused" Fehler

## Phase 7: Health Checks ‚òê

```bash
# Status aller Services
docker compose ps

# Health Details
curl http://localhost:8080/health/detail | jq '.'

# Einzeln:
curl http://localhost:3000/    # Open WebUI
curl http://localhost:8000/health  # vLLM
curl http://localhost:6333/health  # Qdrant
curl http://localhost:8001/health  # TTS
```

Alle sollten Status "ok" zur√ºckgeben.

- [ ] Open WebUI l√§dt (http://localhost:3000)
- [ ] vLLM antwortet
- [ ] Backend antwortet
- [ ] Qdrant antwortet
- [ ] TTS antwortet

## Phase 8: Funktions-Test ‚òê

### Test 1: Chat
```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "default",
    "messages": [{"role": "user", "content": "Hallo"}]
  }'
```
- [ ] Antwort erhalten (ca. 5-15s)

### Test 2: RAG (ohne Dokumente)
```bash
curl -X POST "http://localhost:8080/api/rag/search?query=test&top_k=1"
```
- [ ] Keine Fehler (auch wenn keine Dokumente indiziert)

### Test 3: Whisper STT
```bash
# Einfache Testdatei generieren
echo "test audio" | ffmpeg -f lavfi -i anullsrc=r=16000:cl=mono -t 1 -q:a 9 test.wav

curl -X POST http://localhost:8080/v1/audio/transcriptions \
  -F "file=@test.wav" \
  -F "language=de"
```
- [ ] Transkription liefert (evtl. "test audio" oder √§hnlich)

### Test 4: TTS
```bash
curl -X POST http://localhost:8080/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{"input": "Hallo", "voice": "default"}' \
  --output test.wav

# Audio abspielen
aplay test.wav  # oder ffplay
```
- [ ] Audio-Datei generiert
- [ ] Audio abspielen funktioniert

## Phase 9: Produktivbetrieb ‚úì

### Backups aktivieren
- [ ] T√§gliches Backup der Vektordatenbank einrichten
- [ ] `.env` und Konfiguration sichern

### Monitoring
- [ ] Cron-Job f√ºr Logs-Rotation
- [ ] Regelm√§√üige GPU-Speicher Pr√ºfung

### Sicherheit (optional)
- [ ] Nginx-Reverse-Proxy f√ºr HTTPS konfigurieren
- [ ] Firewall auf nur localhost beschr√§nken

### Dokumentation
- [ ] Notizen √ºber Customization
- [ ] Kontakt-Info f√ºr Support

## Phase 10: Laufender Betrieb ‚úì

**T√§glich:**
```bash
# Status pr√ºfen
docker compose ps

# Logs auf Fehler pr√ºfen
docker compose logs --tail=100 backend
```

**W√∂chentlich:**
```bash
# Backups pr√ºfen
ls -lh data/vectordb/

# GPU-Speicher
nvidia-smi
```

**Nach Updates:**
```bash
docker compose pull
docker compose up -d --build
docker compose ps  # alles ok?
```

---

## Troubleshooting-Reference

| Problem | L√∂sung |
|---------|--------|
| vLLM startet nicht | `docker compose logs -f vllm` ansehen, 5 Min warten |
| Out of Memory | LLM_GPU_MEMORY_UTILIZATION senken oder kleineres Modell |
| HF_TOKEN Fehler | Token in `.env` pr√ºfen, Typo? |
| Port bereits in Verwendung | `lsof -i :3000` pr√ºfen, andere App stoppen |
| "Connection refused" | Service startet noch, `docker compose ps` pr√ºfen |
| Slow Responses | GPU-speicher pr√ºfen (`nvidia-smi`), andere Processes beenden |

---

## Erfolg-Kriterien ‚úÖ

- [ ] Web-Interface l√§dt unter http://localhost:3000
- [ ] Chat funktioniert (Antwort in <20s)
- [ ] Audio-Upload funktioniert
- [ ] Keine Fehler in Logs nach 5 Minuten Start
- [ ] GPU wird genutzt (nvidia-smi zeigt >1% Nutzung)

**Wenn alle Haken drin: Deployment erfolgreich! üéâ**

---

## N√§chste Schritte

1. **Daten-Ingestion**: Gerichtsakten als PDFs hochladen
   ```bash
   curl -X POST http://localhost:8080/api/documents/upload \
     -F "file=@gerichtsakte.pdf" \
     -F "document_type=gerichtsakte"
   ```

2. **Voice Cloning** (optional): Referenz-Audio hochladen
   ```bash
   curl -X POST http://localhost:8001/clone-voice \
     -F "file=@stimme.wav" -F "name=default"
   ```

3. **Integration**: Open WebUI in Klinik-Browser bookmarken
   - Standard-URL: http://192.168.X.X:3000 (vom Host/IP)

4. **Monitoring**: Regelm√§√üige Wartung planen
